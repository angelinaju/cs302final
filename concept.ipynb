{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e16280",
   "metadata": {},
   "source": [
    "Message to Grader: \n",
    "\n",
    "This code doesn't work! Please don't try to run it. This ipnyb is something I wrote because my current implementation in the context of the Spot Mini Mini repository does not work properly and I knew you needed something readable to grade. The majority of the code located in those files are NOT mine, but there some major changes I had to fix in order to get the simulation working. \n",
    "\n",
    "This file is mainly to explain the concept, provide code snippets, and I will try my best to explain what went wrong, why it went wrong, and how I would fix the implementation in the future. \n",
    "\n",
    "Thank you for your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc896d",
   "metadata": {},
   "source": [
    "This first section describes the changes I made to the repository files in order to enable the simulation to properly run. I'm not sure entirely how it broke, but the repository is close to 5 years old, so it was bound to have issues somewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeac49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from spotmicro.util.gui import GUI\n",
    "from spotmicro.GymEnvs.spot_bezier_env import SpotBezierEnv\n",
    "from spotmicro.Kinematics.SpotKinematics import SpotModel\n",
    "from spotmicro.GaitGenerator.Bezier import BezierGait\n",
    "from spotmicro.OpenLoopSM.SpotOL import BezierStepper\n",
    "from spotmicro.spot_env_randomizer import SpotEnvRandomizer\n",
    "from ars_lib.ars import ARSAgent, Normalizer, Policy, ParallelWorker\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe916829",
   "metadata": {},
   "source": [
    "# spot_ars.py line 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also decreased the max_timesteps and eval_freq variables in this file for my less powerful machine\n",
    "\n",
    "# Original code: \n",
    "    # if os.path.exists(models_path + \"/\" + file_name + str(agent_num) +\n",
    "    #                   \"_policy\"):\n",
    "    #     print(\"Loading Existing agent\")\n",
    "    #     agent.load(models_path + \"/\" + file_name + str(agent_num))\n",
    "\n",
    "# Changed code:\n",
    "    pretrained = os.path.join(models_path, f\"spot_ars{agent_num}_policy\")\n",
    "    if os.path.exists(pretrained):\n",
    "         print(\"Loading Existing agent\")\n",
    "        with open(pretrained, 'rb') as f:\n",
    "            old_theta = pickle.load(f, encoding='latin1') # latin1 encoding helps with older formats\n",
    "        new_theta = np.zeros((action_dim, state_dim))   # new matrix for the environment dimensions \n",
    "                                                        # action_dim is num of output actions, state_dim is the new input size\n",
    "        new_theta[:, :old_theta.shape[1]] = old_theta   # copy over pretrained weights\n",
    "        agent.policy.theta = new_theta                  # update agent policies\n",
    "        agent.policy.state_dim = state_dim\n",
    "        \n",
    "    # by adding a ball to the simulation, we add 2 more inputs to the theta (x and y distance to the ball)\n",
    "    # allows the agent to attempt to learn something new while retaining its old knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64892cb2",
   "metadata": {},
   "source": [
    "# spot_ars.py line 166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code: \n",
    "    # for proc_num in range(num_processes):\n",
    "    #     p = mp.Process(target=ParallelWorker, args=(childPipes[proc_num], env, state_dim))\n",
    "    \n",
    "# Changed code:\n",
    "    p = mp.Process(target=ParallelWorker, args=(childPipes[proc_num], state_dim))\n",
    "    \n",
    "# This is a simple change. When parallel workers were being spawned, the environment was being passed\n",
    "# to each worker. However, the pybullet was connecting to the physics server first, and then the environment\n",
    "# being passed wouldn't recognize the physics server when the parallel worker began. More info below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898ce42",
   "metadata": {},
   "source": [
    "# ars.py line 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code:\n",
    "    # nb_states = env.observation_space.shape[0]\n",
    "    # common normalizer\n",
    "    # normalizer = Normalizer(nb_states)\n",
    "    # max_action = float(env.action_space.high[0])\n",
    "    # _ = env.reset()\n",
    "    # n = 0\n",
    "    \n",
    "# Changed Code:\n",
    "import pybullet as p\n",
    "from spotmicro.GymEnvs.spot_bezier_env import spotBezierEnv\n",
    "from spotmicro.spot_env_randomizer import SpotEnvRandomizer\n",
    "\n",
    "    def ParallelWorker(childPipe, nb_states):\n",
    "        p.connect(p.DIRECT)    # p.GUI will open up a window, but rendering isn't possible with all the training\n",
    "        env_randomizer = SpotEnvRandomizer()                # set up environment, environment randomizer was originally included when we had planned\n",
    "        env = spotBezierEnv(render=False, on_rack=False, height_field=False, draw_foot_path=False, # to include rocky terrain\n",
    "                            contacts=True, env_randomizer=env_randomizer)\n",
    "        \n",
    "        env.seed(0)             # worker seed\n",
    "        normalizer = Normalizer(nb_states)\n",
    "        max_action = float(env.action_space.high[0])\n",
    "        _ = env.reset()\n",
    "        n = 0\n",
    "        \n",
    "# in class Policy() line 237, I changed the num_deltas, num_best_deltas, episode_steps, and \n",
    "# exploration_noise variables to smaller numbers so that my laptop would be able to handle it. \n",
    "# The numbers I used were 12, 6, 1000, and 0.1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f8d51",
   "metadata": {},
   "source": [
    "# spot_ars_eval.py line 174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16404c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code: \n",
    "    # if os.path.exists(models_path + \"/\" + file_name + str(agent_num) +\n",
    "    #                   \"_policy\"):\n",
    "    #     print(\"Loading Existing agent\")\n",
    "    #     agent.load(models_path + \"/\" + file_name + str(agent_num))\n",
    "    #     agent.policy.episode_steps = np.inf\n",
    "    #     policy = agent.policy\n",
    "    \n",
    "# Changed code: \n",
    "    policy_path = os.path.join(models_path, f\"{file_name}{agent_num}_policy\")\n",
    "    if os.path.exists(policy_path):\n",
    "        print(f\"Loading Existing policy from {policy_path}\")\n",
    "        try: \n",
    "            with open(policy_path,\"rb\") as f:\n",
    "                theta = pickle.load(f, encoding='latin1')\n",
    "            agent.policy.theta = theta\n",
    "            agent.policy.episode_steps = np.inf\n",
    "            policy = agent.policy\n",
    "            print(\"Policy loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load policy: {e}\")\n",
    "    \n",
    "    # Nothing super new here besides the latin1 encoding and the check to ensure the policy was \n",
    "    # loaded correctly while trying to get presaved policies to work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d450980",
   "metadata": {},
   "source": [
    "This next section is included because while I attempted to add a ball to the repository's premade environment, I unfortunately kept running into bugs and was unable to successfully force the pretrained model to recognize the ball. Some of the code below is code I attempted to compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in spot_bezier_env.py in __init__\n",
    "import random as r \n",
    "randX = r.randint(0, 10)\n",
    "randZ = r.randint(0, 10)\n",
    "\n",
    "self.ball_uid = self.pybullet_client.loardURDF(  # adds a sphere shape object directly in front of the robot. \n",
    "    \"sphere_small.urdf\", \n",
    "    startPos=[randX, 0, randZ]  # randomized ball location\n",
    "    globalScaling = 0.1\n",
    "    p.setGravity(0, 0, -10)    #to help the ball stay in place..\n",
    " )\n",
    "\n",
    "def get_ball_pos_to_robot(self):            # defines the relative position between the ball and the robot\n",
    "    robotPos = np.array(self.spot.GetBasePosition())\n",
    "    ballPos = np.array(self._pybullet_client.getBasePositionAndOrientation(self.ball_uid)[0])\n",
    "    return ballPos - robotPos\n",
    "\n",
    "# to get the robot to be aware of the ball position, we need to make the policy aware of the new numbers\n",
    "def _get_observation(self):\n",
    "    obs = super()._get_observation()\n",
    "    relativeBallPos = self.get_ball_pos_to_robot()\n",
    "    return np.concatenate([obs, relativeBallPos])\n",
    "\n",
    "# in the reward function, I will do some calculations for moving towards the ball\n",
    "ballDistance = np.linalg.norm(self.get_ball_pos_to_robot())  # euclidean distance between 2 poitns in the x y plane\n",
    "reward = reward + -1.0 * ballDistance                        # negative reward for moving away, positive reward for moving closer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfb0ca",
   "metadata": {},
   "source": [
    "Because the goal is to get the robot to learn that following the ball is the new goal instead of just walking, we have to retrain to some extent. But I don't want to retrain the model from scratch, so I tried to edit the agent logic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
