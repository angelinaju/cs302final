{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00525b0b",
   "metadata": {},
   "source": [
    "## PPO Implementation from Scratch\n",
    "NOTE: it is an early phases of training so it will look chaotic when running. That is normal and to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c77928",
   "metadata": {},
   "source": [
    "Additional Info: this model is working to train the spot mini mini quadruped in the pybullet simulated environment how to turn towards a randomly generated ball and walk towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "from spotmicro import spot_gym_env\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.normal as Normal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13883ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the PPO memory class\n",
    "# the PPO memory class is used to store the experience of the agent\n",
    "# the experience is stored in the form of states, actions, rewards, dones, probs and vals\n",
    "# honestly didn't end up using this class, but it is here for reference\n",
    "class PPOMemory:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "# defining the policy network\n",
    "# the policy network is a neural network that takes the state as input\n",
    "# and outputs the mean and standard deviation of the action distribution\n",
    "class PolicyNet:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # defining the policy network as a sequential model\n",
    "        # the policy network consists of three fully connected layers with ReLU activation functions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # Output the mean of the action distribution\n",
    "        )\n",
    "        # log_std is a learnable parameter that represents the standard deviation of the action distribution\n",
    "        # it is initialized to zero and will be learned during training\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "        # optimizer for the policy network\n",
    "        # Adam optimizer is used for training the policy network\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    # defining the forward pass of the policy network\n",
    "    def forward(self, state):\n",
    "        mean = self.model(state)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "# defining the value network\n",
    "# the value network is a neural network that takes the state as input\n",
    "# and outputs the value of the state\n",
    "class ValueNet:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # defining the value network as a sequential model\n",
    "        # the value network consists of three fully connected layers with ReLU activation functions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output a single value for the state value\n",
    "        )\n",
    "\n",
    "        # optimizer for the value network\n",
    "        # Adam optimizer is used for training the value network\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    # defining the forward pass of the value network\n",
    "    # the forward pass takes the state as input and outputs the value of the state\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "    \n",
    "# defining the PPO agent\n",
    "# the PPO agent is responsible for interacting with the environment and learning from the experience\n",
    "# the PPO agent uses the policy and value networks to choose actions and estimate the value of the states\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim, hyperparams):\n",
    "        self.trajectory = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'dones': [],\n",
    "            'probs': [],\n",
    "            'vals': []\n",
    "        }\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.policy = PolicyNet(input_dim, output_dim)\n",
    "        self.value = ValueNet(input_dim)\n",
    "        self.gamma = hyperparams['gamma']\n",
    "        self.clip_ratio = hyperparams['epsilon']\n",
    "        self.epochs = hyperparams['epochs']\n",
    "        self.batch_size = hyperparams['batch_size']\n",
    "        self.memory = PPOMemory(self.batch_size)\n",
    "\n",
    "    # don't think i ended up using this function, but it is here for reference\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    # store the trajectory of the agent in the environment\n",
    "    # the trajectory is a list of states, actions, rewards, dones, probs and vals\n",
    "    def store_trajectory(self, state, action, probs, vals, reward, done):\n",
    "        self.trajectory['states'].append(state)\n",
    "        self.trajectory['actions'].append(action)\n",
    "        self.trajectory['probs'].append(probs)\n",
    "        self.trajectory['vals'].append(vals)\n",
    "        self.trajectory['rewards'].append(reward)\n",
    "        self.trajectory['dones'].append(done)\n",
    "\n",
    "    # clear the trajectory after each episode\n",
    "    # to avoid memory leaks and to start a new trajectory\n",
    "    def clear_trajectory(self):\n",
    "        self.trajectory['states'] = []\n",
    "        self.trajectory['actions'] = []\n",
    "        self.trajectory['probs'] = []\n",
    "        self.trajectory['vals'] = []\n",
    "        self.trajectory['rewards'] = []\n",
    "        self.trajectory['dones'] = []\n",
    "\n",
    "    # compute the discounted rewards for the trajectory\n",
    "    # the discounted rewards are used to calculate the advantage of the actions taken by the agent\n",
    "    # the advantage is the difference between the expected reward and the actual reward\n",
    "    # the advantage is used to update the policy and value networks\n",
    "    def compute_advantage(self):\n",
    "        advantage = 0\n",
    "        advantages = []\n",
    "\n",
    "        for step in reversed(self.trajectory['rewards']):\n",
    "            advantage+= self.gamma*step\n",
    "            advantages.append(advantage)\n",
    "\n",
    "        advantages.reverse()\n",
    "\n",
    "        self.trajectory['advantages'] = advantages\n",
    "\n",
    "    # where the agent chooses an action based on the current state\n",
    "    # the action is sampled from a normal distribution with mean and std\n",
    "    def choose_action(self, state):\n",
    "        mean, std = self.policy.forward(state)\n",
    "        dist = Normal.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        prob = dist.log_prob(action).sum(dim=-1, keepdim=True).unsqueeze(0)\n",
    "        val = self.value.forward(state).item()\n",
    "        action = action.squeeze()\n",
    "        action *= 0.5\n",
    "\n",
    "        return action, prob, val\n",
    "    \n",
    "    # generating mini-batches of the trajectory\n",
    "    # for training the policy and value networks\n",
    "    def generate_batches(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        probs = []\n",
    "        values = []\n",
    "        advantages = []\n",
    "\n",
    "        # shuffling the indices of the trajectory for mini-batch training\n",
    "        # without actually shuffling the trajectory\n",
    "        n_steps = len(self.trajectory['states'])\n",
    "        indices = []\n",
    "        for i in range(n_steps):\n",
    "            indices.append(i)\n",
    "        indices = np.array(indices, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # splitting the indices into mini-batches\n",
    "        indices = np.array_split(indices, self.batch_size)\n",
    "\n",
    "        # creating mini-batches of the trajectory\n",
    "        for batch in indices:\n",
    "            states.append([self.trajectory['states'][i] for i in batch])\n",
    "            actions.append([self.trajectory['actions'][i] for i in batch])\n",
    "            rewards.append([self.trajectory['rewards'][i] for i in batch])\n",
    "            dones.append([self.trajectory['dones'][i] for i in batch])\n",
    "            probs.append([self.trajectory['probs'][i] for i in batch])\n",
    "            values.append([self.trajectory['vals'][i] for i in batch])\n",
    "            advantages.append([self.trajectory['advantages'][i] for i in batch])\n",
    "\n",
    "        return states, actions, rewards, dones, probs, values, advantages\n",
    "        \n",
    "    # training the policy and value networks using the mini-batches of the trajectory\n",
    "    # the training is done using the PPO algorithm\n",
    "    def learn(self):\n",
    "        self.compute_advantage()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # generating mini-batches of the trajectory\n",
    "            states, actions, _, _, probs, _, advantages = self.generate_batches()\n",
    "\n",
    "            for batch in range(len(states)):\n",
    "                states_mini_batch = states[batch]\n",
    "                probs_mini_batch = probs[batch]\n",
    "                advantages_mini_batch = advantages[batch]\n",
    "            \n",
    "\n",
    "                losses = []\n",
    "                values = []\n",
    "\n",
    "                for i in range(len(states_mini_batch)):\n",
    "                    state = states_mini_batch[i]\n",
    "                    mean, std = self.policy.forward(state)\n",
    "                    dist = Normal.Normal(mean, std)\n",
    "\n",
    "                    # entropy controls the exploration of the agent\n",
    "                    # the higher the entropy, the more exploration\n",
    "                    entropy = dist.entropy().mean()\n",
    "                    action = dist.sample()\n",
    "                    action = action.squeeze()\n",
    "                    action *= 0.5\n",
    "                    new_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "\n",
    "                    # calculate the ratio of the new and old probabilities\n",
    "                    ratio = torch.exp(new_prob - probs_mini_batch[i])\n",
    "\n",
    "                    # calculate the surrogate loss\n",
    "                    surrogate_loss = ratio * advantages_mini_batch[i]\n",
    "                    clipped_ratio = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio)\n",
    "                    loss = torch.min(torch.tensor([surrogate_loss, clipped_ratio * advantages_mini_batch[i]]))\n",
    "                    losses.append(loss)\n",
    "\n",
    "                    # calculate the value loss\n",
    "                    value = self.value.forward(state)\n",
    "                    values.append(value)\n",
    "\n",
    "                # taking the mean of the losses and values\n",
    "                mean_loss = torch.mean(torch.tensor(losses, requires_grad=True))\n",
    "                policy_loss = -mean_loss\n",
    "                value_loss = torch.mean((torch.tensor(values, requires_grad=True) - torch.tensor(advantages_mini_batch)) ** 2)\n",
    "                #total_loss = policy_loss + 0.5*value_loss - 0.01*entropy\n",
    "\n",
    "                # update the policy network\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy.optimizer.step()\n",
    "                \n",
    "                # update the value network\n",
    "                self.value.optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'gamma': 0.9,\n",
    "    'epsilon': 0.2,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "# initialize the env\n",
    "env = spot_gym_env.spotGymEnv()\n",
    "env.render(mode='human')\n",
    "env.reset()\n",
    "env.seed(0)\n",
    "\n",
    "# initialize the agent\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.shape[0], hyperparams)\n",
    "\n",
    "# training the agent\n",
    "# the more episodes the better the agent will perform\n",
    "# we're doing 200 episodes here\n",
    "for episode in range(1, 200):\n",
    "    # getting the initial state of the environment\n",
    "    # and converting it to a tensor\n",
    "    obs = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float)\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    agent.clear_trajectory()\n",
    "\n",
    "    # running the agent in the environment\n",
    "    # the agent will take actions in the environment until it reaches the max steps\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_obs = torch.tensor(next_obs, dtype=torch.float)\n",
    "        \n",
    "        # storing the trajectory of the agent, which includes all the info gained from the step in the environment\n",
    "        # this will be used to update the policy and value networks\n",
    "        agent.store_trajectory(obs, action, prob, val, reward, done)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # setting the next observation as the current observation\n",
    "        obs = next_obs\n",
    "\n",
    "    # learning from the trajectory of the agent\n",
    "    agent.learn()\n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
